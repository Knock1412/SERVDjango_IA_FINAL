# SERVDjango_IA_FINAL

# ğŸ“š Projet IA IMPERIATEC

Application complÃ¨te dâ€™Intelligence Artificielle permettant :
- ğŸ“„ Lâ€™analyse automatique et le rÃ©sumÃ© de documents PDF.
- ğŸ’¬ Une interface de chat IA conversationnelle basÃ©e sur les contenus traitÃ©s.
- ğŸ§  La reformulation intelligente des rÃ©ponses gÃ©nÃ©rÃ©es.

---

## ğŸ§± Structure du projet

```bash
backend/
â”œâ”€â”€ ia_backend/
â”‚   â”œâ”€â”€ ask_engine.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ pdf_utils.py
â”‚   â”‚   â”œâ”€â”€ chat_memory.py
â”‚   â”‚   â”œâ”€â”€ ollama_gateway.py
â”‚   â”‚   â”œâ”€â”€ backup_service.py
â”‚   â”œâ”€â”€ views.py
â”‚   â”œâ”€â”€ tasks.py

Front-IA-IMPERIATEC/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ chat.tsx
â”œâ”€â”€ assets/
â”œâ”€â”€ package.json
ğŸš€ DÃ©marrage rapide

ğŸ”§ PrÃ©requis
Docker & Docker Compose
Node.js & npm
Python 3.10+
Ollama (local ou dans Docker)
ğŸ³ DÃ©marrage backend avec Docker

# 1. Construire les images
docker compose build

# 2. Lancer les services en arriÃ¨re-plan
docker compose up -d

# 3. Voir les logs du worker Celery
docker compose logs -f worker

# 4. Voir les logs du backend Django
docker compose logs -f web

# 5. Stopper tous les services
docker compose down


ğŸ’» Lancer le frontend React Native

# Aller dans le dossier frontend
cd Front-IA-IMPERIATEC

# Installer les dÃ©pendances
npm install

# Lancer l'application (Expo)
npx expo start
Assure-toi que lâ€™URL de lâ€™API (API_BASE_URL) est bien dÃ©finie dans le code React Native (ex. chat.tsx), comme :

const API_BASE_URL = "http://192.168.10.121:8000";
ğŸ§  FonctionnalitÃ©s IA

RÃ©sumÃ© de PDF par bloc â¡ï¸ synthÃ¨se globale
Chat IA personnalisÃ© avec mÃ©moire de session
Reformulation automatique via prompt dÃ©diÃ©
DÃ©tection de langue et traduction automatique (EN â¡ï¸ FR)
ğŸ“š Technologies principales

Composant	RÃ´le
Django / DRF	Backend API
Celery + Redis	Traitement asynchrone
SQLite / JSON	Cache local rapide
SentenceTransformer	Embedding sÃ©mantique
CrossEncoder (MS MARCO)	Re-ranking des rÃ©sultats
Ollama + LLaMA3	GÃ©nÃ©ration des rÃ©ponses IA
React Native + Expo	Interface utilisateur mobile
ğŸ“¦ Installation des modÃ¨les Ollama

Exemple :

ollama pull mistral
â“ Ã€ propos

Ce projet a Ã©tÃ© conÃ§u pour intÃ©grer lâ€™IA dans une application mobile intuitive, tout en exploitant la puissance des LLMs localement avec une grande efficacitÃ©.