import os
import json
import logging
import uuid
import torch
import numpy as np
import time
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer, util, CrossEncoder

# --- Imports sp√©cifiques backend IA ---
from ia_backend.services.ollama_gateway import generate_ollama
from ia_backend.services.metadata_db import (
    find_nearest_pdf_by_embedding,      # FAISS
    find_documents_by_keyword,          # FTS5 (gard√© si besoin)
    find_documents_by_keyword_semantic
)


from ia_backend.services.chat_memory import save_interaction

# --- Initialisation logging et mod√®les ---
logger = logging.getLogger(__name__)
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")


# ---------------------------------------------------------------------------
#    Utilitaire‚ÄØ: classification LLM ‚Äúg√©n√©rale/pr√©cise‚Äù + score de confiance
# ---------------------------------------------------------------------------
from functools import lru_cache
import concurrent.futures
from typing import Tuple, Dict
import numpy as np

# Cache pour les embeddings de questions (r√©duit les appels LLM)
QUESTION_EMBEDDINGS_CACHE = lru_cache(maxsize=1000)

# Mod√®le lightweight pour pr√©-classification
FAST_CLASSIFIER = SentenceTransformer("paraphrase-MiniLM-L6-v2")

# Exemples de questions pr√©-classifi√©es pour few-shot learning
# Exemples de questions pr√©-classifi√©es pour few-shot learning
PRECLASSIFIED_EXAMPLES = [
    # Questions G√âN√âRALES
    ("Liste des documents sur la fiscalit√©", "g√©n√©rale"),
    ("R√©sumez les rapports financiers 2023", "g√©n√©rale"),
    ("Quels documents traitent des politiques √©ducatives ?", "g√©n√©rale"),
    ("Quels sont les th√®mes abord√©s dans les documents r√©cents ?", "g√©n√©rale"),
    ("Montre-moi les documents li√©s √† la transformation num√©rique", "g√©n√©rale"),
    ("Quels fichiers abordent l'enseignement √† distance ?", "g√©n√©rale"),
    ("Y a-t-il des documents qui parlent d'√©thique en IA ?", "g√©n√©rale"),
    ("Quels rapports concernent les innovations p√©dagogiques ?", "g√©n√©rale"),

    # Questions PR√âCISES
    ("Quel est l'article sur les imp√¥ts locaux ?", "pr√©cise"),
    ("Page 42 du document X", "pr√©cise"),
    ("Quelles sont les conclusions du rapport sur la visioconf√©rence ?", "pr√©cise"),
    ("Combien de pages contient le document sur le Cartable √âlectronique ?", "pr√©cise"),
    ("Quelle est la date de publication du PDF sur la nanobureautique ?", "pr√©cise"),
    ("Quels logiciels sont mentionn√©s dans le bloc 3 du document X ?", "pr√©cise"),
    ("Quelles sont les critiques soulev√©es dans le document sur les TICE ?", "pr√©cise"),
    ("Quelle m√©thode p√©dagogique est d√©crite dans le document sur le MO5 ?", "pr√©cise")
]


def classify_question_with_score_v2(question: str) -> Tuple[str, float]:
    """
    Version optimis√©e avec fallback intelligent et pr√©-classification
    """
    # √âtape 1: Pr√©-classification rapide avec embedding (√©vite 60% des appels LLM)
    pre_class, pre_conf = fast_preclassify(question)
    if pre_conf > 0.85:  # Seuil de confiance √©lev√©
        return (pre_class, pre_conf)

    # √âtape 2: Appel LLM seulement si n√©cessaire
    llm_class, llm_conf = call_llm_classifier(question)
    
    # √âtape 3: Fusion intelligente des r√©sultats
    final_class, final_conf = combine_results(
        pre_class, pre_conf,
        llm_class, llm_conf
    )
    
    return (final_class, final_conf)

@QUESTION_EMBEDDINGS_CACHE
def fast_preclassify(question: str) -> Tuple[str, float]:
    """
    Classification rapide avec similarit√© s√©mantique sur exemples connus
    """
    question_emb = FAST_CLASSIFIER.encode(question)
    examples_embs = [FAST_CLASSIFIER.encode(ex[0]) for ex in PRECLASSIFIED_EXAMPLES]
    
    similarities = util.pytorch_cos_sim(question_emb, examples_embs)[0]
    max_idx = similarities.argmax().item()
    
    if similarities[max_idx] > 0.75:  # Seuil de similarit√©
        return (PRECLASSIFIED_EXAMPLES[max_idx][1], float(similarities[max_idx]))
    
    return ("inconnu", 0.0)

def call_llm_classifier(question: str) -> Tuple[str, float]:
    """
    Appel LLM optimis√© avec timeout et retry
    """
    prompt = build_few_shot_prompt(question)
    
    for attempt in range(2):  # 2 tentatives max
        try:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    generate_ollama,
                    prompt=prompt,
                    num_predict=80,  # R√©duit pour la classification
                    
                )
                response = future.result(timeout=3.0)
                
            data = json.loads(response.strip())
            return validate_response(data)
        
        except Exception as e:
            logger.warning(f"Attempt {attempt+1} failed: {str(e)}")
            continue
            
    return ("pr√©cise", 0.5)  # Fallback conservateur

def build_few_shot_prompt(question: str) -> str:
    """
    Prompt avec exemples pour meilleure consistance
    """
    examples_str = "\n".join(
        f"- Exemple {i+1} ({type}): {q}"
        for i, (q, type) in enumerate(PRECLASSIFIED_EXAMPLES)
    )
    
    return f"""[INST]
Tu es un classifieur de questions. Voici des exemples :

{examples_str}

Classifie cette nouvelle question :

QUESTION: {question}

R√©ponds UNIQUEMENT en JSON valide :
{{
  "type": "g√©n√©rale"|"pr√©cise",
  "confiance": 0.0-1.0,
  "raison": "explication courte"
}}
[/INST]"""

def combine_results(
    pre_class: str, pre_conf: float,
    llm_class: str, llm_conf: float
) -> Tuple[str, float]:
    """
    Combine intelligemment la pr√©-classification et la classification LLM
    """
    if llm_conf >= 0.75:
        return (llm_class, llm_conf)

    if pre_class == llm_class:
        # Accord entre classifieur rapide et LLM
        avg_conf = round((pre_conf + llm_conf) / 2, 3)
        return (llm_class, avg_conf)

    # Divergence : on fait confiance au LLM s‚Äôil d√©passe un certain seuil
    if llm_conf >= 0.6:
        return (llm_class, llm_conf)
    else:
        return (pre_class, pre_conf)

def validate_response(data: Dict) -> Tuple[str, float]:
    """
    Validation robuste de la r√©ponse LLM
    """
    if not isinstance(data, dict):
        raise ValueError("Invalid JSON format")
    
    q_type = data.get("type", "").lower()
    if q_type not in {"g√©n√©rale", "pr√©cise", "precise"}:
        raise ValueError("Invalid question type")
    
    confiance = min(max(float(data.get("confiance", 0.5)), 1.0), 0.0)
    
    # Post-processing bas√© sur l'explication
    if "raison" in data:
        if "document sp√©cifique" in data["raison"].lower():
            q_type = "pr√©cise"
        elif "plusieurs" in data["raison"].lower():
            q_type = "g√©n√©rale"
    
    return (q_type, round(confiance, 2))





# ---------------------------------------------------------------   ------------
#     Utilitaire‚ÄØ: charger tous les blocs d‚Äôun job_id (fichier par PDF)
# ---------------------------------------------------------------------------
def load_all_blocks(entreprise: str, job_id: str) -> List[Tuple[str, Dict]]:
    """
    Charge tous les blocs de r√©sum√© depuis le cache JSON pour une entreprise et un job donn√©s.
    
    Args:
        entreprise: Nom de l'entreprise
        job_id: Identifiant du job
        
    Returns:
        Liste de tuples (r√©sum√©, m√©tadonn√©es) pour chaque bloc valide
        
    Raises:
        FileNotFoundError: Si le dossier sp√©cifi√© n'existe pas
    """
    folder_path = os.path.join("cache_json", "save_summaryblocks", entreprise, job_id)
    logger.info(f"Chargement des blocs depuis le dossier: {folder_path}")
    
    if not os.path.exists(folder_path):
        logger.error(f"Dossier introuvable: {folder_path}")
        raise FileNotFoundError(f"Dossier introuvable : {folder_path}")

    blocks: List[Tuple[str, Dict]] = []
    valid_files = 0
    skipped_files = 0
    
    for filename in sorted(os.listdir(folder_path)):
        # Filtrage des fichiers JSON de blocs
        if not filename.startswith("bloc_") or not filename.endswith(".json"):
            skipped_files += 1
            continue
            
        full_path = os.path.join(folder_path, filename)
        try:
            with open(full_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                
            if "embedding" not in data:
                logger.warning(f"Fichier {filename} ignor√© - embedding manquant")
                skipped_files += 1
                continue

            # V√©rifie que l'embedding est bien une liste de 384 floats
            emb = data["embedding"]
            if isinstance(emb, str):
                try:
                    emb = json.loads(emb)
                except Exception as e:
                    logger.warning(f"{filename} - embedding JSON mal form√©: {e}")
                    skipped_files += 1
                    continue

            if not isinstance(emb, list) or len(emb) != 384:
                logger.warning(f"{filename} - embedding invalide (type: {type(emb)}, taille: {len(emb) if isinstance(emb, list) else 'N/A'})")
                skipped_files += 1
                continue

            # Construction des m√©tadonn√©es
            meta = {
                "source": filename,
                "score": data.get("score", 0),
                "translated": data.get("translated", False),
                "embedding": emb
            }
            blocks.append((data.get("summary", ""), meta))
            valid_files += 1
            
        except json.JSONDecodeError as e:
            logger.error(f"Erreur de d√©codage JSON dans {filename}: {str(e)}")
            skipped_files += 1
        except Exception as e:
            logger.error(f"Erreur inattendue lors du traitement de {filename}: {str(e)}")
            skipped_files += 1

    logger.info(
        f"Chargement termin√© - {valid_files} blocs valides, "
        f"{skipped_files} fichiers ignor√©s/erron√©s"
    )
    return blocks



# ---------------------------------------------------------------------------
#     Utilitaire‚ÄØ: trouver les blocs les plus pertinents (pipeline IA PDF)
# ---------------------------------------------------------------------------
def find_relevant_blocks(
    question: str,
    blocks: List[Tuple[str, Dict]],
    top_k: int = 5,
    relevance_threshold: float = 0.4
) -> List[Dict]:
    """
    Trouve les blocs les plus pertinents pour une question donn√©e en combinant:
    - Similarit√© s√©mantique (70%)
    - Score de qualit√© du bloc (30%)
    - Re-ranking crois√©
    
    Args:
        question: Question √† laquelle r√©pondre
        blocks: Liste des blocs √† analyser
        top_k: Nombre maximum de r√©sultats √† retourner
        relevance_threshold: Seuil minimal de pertinence
        
    Returns:
        Liste des blocs pertinents avec leurs m√©tadonn√©es
    """
    # Phase 1: Pr√©paration des embeddings
    candidate_blocks = []
    embeddings = []
    logger.info(f"D√©but de recherche sur {len(blocks)} blocs - top_k={top_k}")
    
    for summary, meta in blocks:
        if "embedding" in meta:
            embeddings.append(meta["embedding"])
            candidate_blocks.append((summary, meta))
    
    if not embeddings:
        logger.warning("Aucun embedding valide trouv√© - recherche annul√©e")
        return []

    # Phase 2: Calcul des similarit√©s
    logger.debug("Conversion des embeddings en tenseurs...")
    block_embs = torch.tensor(np.stack(embeddings), dtype=torch.float32).to("cpu")
    question_emb = model.encode(question, convert_to_tensor=True).to(torch.float32).to("cpu")
    
    logger.info("Calcul des scores de similarit√©...")
    similarities = util.pytorch_cos_sim(question_emb, block_embs)[0]
    
    # Phase 3: Combinaison des scores
    scored_blocks = []
    debug_scores = []
    
    for i, (_, meta) in enumerate(candidate_blocks):
        sim_score = similarities[i].item()
        quality_score = meta.get("score", 0)
        combined_score = 0.7 * sim_score + 0.3 * quality_score
        
        debug_scores.append({
            "source": meta["source"],
            "sim_score": round(sim_score, 4),
            "quality_score": round(quality_score, 4),
            "combined_score": round(combined_score, 4)
        })
        
        logger.debug(
            f"Bloc {meta['source']} - "
            f"Similarit√©: {sim_score:.4f}, "
            f"Qualit√©: {quality_score:.4f}, "
            f"Combined: {combined_score:.4f}"
        )
        
        if combined_score >= relevance_threshold:
            scored_blocks.append((i, combined_score))

    logger.info(
        f"{len(scored_blocks)} blocs d√©passent le seuil de {relevance_threshold} "
        f"sur {len(candidate_blocks)} analys√©s"
    )

    # Phase 4: S√©lection initiale
    if not scored_blocks:
        logger.warning("Aucun bloc ne d√©passe le seuil - utilisation des meilleurs scores")
        scored_blocks = [
            (i, 0.7 * similarities[i].item() + 0.3 * candidate_blocks[i][1].get("score", 0))
            for i in range(len(candidate_blocks))
        ]
    
    # Tri et s√©lection des top_k
    scored_blocks.sort(key=lambda x: x[1], reverse=True)
    top_indices = [i for i, _ in scored_blocks[:top_k]]
    
    # Phase 5: Re-ranking crois√©
    logger.info("Application du re-ranking crois√©...")
    cross_inputs = [(question, candidate_blocks[i][0]) for i in top_indices]
    rerank_scores = reranker.predict(cross_inputs)
    
    # Construction des r√©sultats finaux
    results = []
    for rank, (idx, rerank_score) in enumerate(
        sorted(zip(top_indices, rerank_scores), key=lambda x: x[1], reverse=True),
        1
    ):
        summary, meta = candidate_blocks[idx]
        scores = debug_scores[idx]
        
        logger.info(
            f"Bloc s√©lectionn√© (#{rank}): {meta['source']}\n"
            f"  - Similarit√©: {scores['sim_score']:.4f}\n"
            f"  - Qualit√©: {scores['quality_score']:.4f}\n"
            f"  - Score combin√©: {scores['combined_score']:.4f}\n"
            f"  - Re-rank score: {float(rerank_score):.4f}"
        )
        
        results.append({
            "text": summary,
            "source": meta["source"],
            "debug_scores": scores
        })

    return results[:top_k]

# ---------------------------------------------------------------------------
#    Utilitaire‚ÄØ: prompt pour synth√®se multi-documents (mode g√©n√©rale)
# ---------------------------------------------------------------------------
def build_summary_prompt_from_metadata(question: str, docs: list) -> str:
    doc_lines = []
    for filename, resume in docs:
        doc_lines.append(f"- {filename}: {resume[:120]}...")
    docs_str = "\n".join(doc_lines)
    return f"""[INST]
Voici la question de l'utilisateur :
{question}

Voici la liste des documents pertinents avec leur r√©sum√© :
{docs_str}

R√©ponds par une synth√®se claire :
- Si la question concerne plusieurs documents, cite-les bri√®vement.
- Si le sujet n‚Äôest trait√© dans aucun document, indique-le poliment.
- 2 √† 3 phrases maximum.
[/INST]"""

# ---------------------------------------------------------------------------
#    Utilitaires prompts (PDF pipeline)
# ---------------------------------------------------------------------------
def build_prompt(question: str, selected_blocks: List[Dict]) -> str:
    parts = [f"--- Bloc {i+1} ({b['source']}) ---\n{b['text']}" for i, b in enumerate(selected_blocks)]
    context = "\n\n".join(parts)
    return f"""[INST]
Tu es un assistant IA expert et synth√©tique. R√©ponds en deux √† trois phrases claires et pr√©cises √† la question suivante, en t'appuyant exclusivement sur les extraits fournis.

QUESTION :
{question}

EXTRAITS :
{context}

INSTRUCTIONS :
- R√©ponds en deux √† trois phrases maximum, sans introduction, ni structure acad√©mique.
- Si l‚Äôinformation n‚Äôest pas pr√©sente, dis-le poliment (‚ÄúNon mentionn√© dans le document.‚Äù).
- Corrige les √©ventuelles fautes dans la question.
[/INST]"""

def build_reformulation_prompt(question: str, selected_blocks: List[Dict]) -> str:
    parts = [f"--- Bloc {i+1} ({b['source']}) ---\n{b['text']}" for i, b in enumerate(selected_blocks)]
    context = "\n\n".join(parts)
    return f"""[INST]
Tu es un assistant IA expert et synth√©tique. L‚Äôutilisateur demande une reformulation de ta r√©ponse pr√©c√©dente √† la m√™me question.
R√©dige une r√©ponse diff√©rente, uniquement √† partir des extraits fournis, sans te r√©p√©ter.

QUESTION :
{question}

EXTRAITS :
{context}

R√àGLES :
- 1 √† 3 phrases claires, sans pr√©ambule.
- Ne redis pas la m√™me chose.
- Si l‚Äôinformation n‚Äôest pas pr√©sente, dis-le poliment (‚ÄúNon mentionn√© dans le document.‚Äù).
[/INST]"""

# ---------------------------------------------------------------------------
#    FONCTION CENTRALE : G√©n√©ration de la r√©ponse IA
# ---------------------------------------------------------------------------
def generate_answer(
    question: str,
    blocks: List[Tuple[str, Dict]],
    job_id: str = None,
    session_id: str = None,
    user_id: str = None,
    entreprise: str = "Entreprise_S3_Test",
    reformule: bool = False,
    general: bool = False
) -> str:
    total_start = time.time()

    # --- 1. Classifier la question (g√©n√©rale ou pr√©cise) ---
    q_type, confiance = classify_question_with_score_v2(question)
    logger.info(f"üß† Type de question d√©tect√© : {q_type.upper()} (confiance={confiance})")

    if q_type == "g√©n√©rale":
        logger.info("üü° Branche G√âN√âRALE ‚Äî recherche hybride (FTS+embeddings) sur mots_cles/themes")
        results = find_documents_by_keyword_semantic(
            question, entreprise, job_id, encode_text_fn=model.encode
        )
        if not results:
            return "Aucun document ne correspond √† cette th√©matique."

        prompt = build_summary_prompt_from_metadata(question, results)
        gen_start = time.time()
        answer = generate_ollama(
            prompt=prompt,
            num_predict=400,
            models=["llama3:instruct"],
            temperature=0.4
        ).strip()
        gen_time = time.time() - gen_start
        logger.info(f"‚è±Ô∏è Temps g√©n√©ration (g√©n√©rale) : {gen_time:.2f}s")
        blocks_used = [doc[0] for doc in results]  # liste des filenames

    else:
        logger.info("üîµ Branche PR√âCISE ‚Äî recherche vectorielle du PDF cible")
        question_emb = model.encode(question).tolist()
        pdf_matches = find_nearest_pdf_by_embedding(question_emb, entreprise, job_id)

        if not pdf_matches:
            logger.warning("Aucun document pertinent trouv√© via embeddings.")
            return "Je n‚Äôai pas trouv√© de document correspondant √† votre question."

        # On prend uniquement le fichier avec le meilleur score
        pdf_filename, score = pdf_matches[0]
        logger.info(f"üìÇ Document s√©lectionn√© via FAISS : {pdf_filename} (score={score:.4f})")

        # Charger uniquement les blocs li√©s √† ce PDF
        all_blocks = load_all_blocks(entreprise, job_id)
        filtered_blocks = [
            b for b in all_blocks if b[1].get("pdf_filename") == pdf_filename
        ]


        selected = find_relevant_blocks(question, filtered_blocks)
        retrieval_time = time.time() - total_start
        logger.info(f"ASK ‚è±Ô∏è Temps s√©lection blocs (retrieval+r√©rank) : {retrieval_time:.2f}s")

        if not selected:
            logger.info("üß† Aucun bloc pertinent ‚Äî pas d'information.")
            return "Je n‚Äôai pas trouv√© cette information dans les documents analys√©s."

        logger.info(f"üß† Prompt d√©clench√© : üìÑ PDF ({'reformulation' if reformule else 'standard'})")
        prompt = build_reformulation_prompt(question, selected) if reformule else build_prompt(question, selected)
        gen_start = time.time()
        answer = generate_ollama(
            prompt=prompt,
            num_predict=550,
            models=["llama3:instruct"],
            temperature=0.3
        ).strip()
        gen_time = time.time() - gen_start
        logger.info(f"‚è±Ô∏è Temps g√©n√©ration (pdf) : {gen_time:.2f}s")
        blocks_used = [b["source"] for b in selected]

    total_time = time.time() - total_start
    logger.info(f"‚úÖ R√©ponse g√©n√©r√©e en {total_time:.2f}s (total)")

    if session_id is None:
        session_id = str(uuid.uuid4())
    try:
        save_interaction(
            session_id=session_id,
            question=question,
            answer=answer,
            blocks_used=blocks_used,
            job_id=job_id,
            user_id=user_id
        )
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde de l'interaction chat : {e}")

    return answer

# ---------------------------------------------------------------------------
# FIN DU MODULE ask_engine.py
# ---------------------------------------------------------------------------


