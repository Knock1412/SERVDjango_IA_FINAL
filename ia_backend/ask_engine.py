import os
import json
import logging
import uuid
import torch
import numpy as np
import time
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer, util, CrossEncoder

# --- Imports sp√©cifiques backend IA ---
from ia_backend.services.ollama_gateway import generate_ollama
from ia_backend.services.metadata_db import (
    find_nearest_pdf_by_embedding,
    find_documents_by_keyword
)
from ia_backend.services.chat_memory import save_interaction

# --- Initialisation logging et mod√®les ---
logger = logging.getLogger(__name__)
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")


# ---------------------------------------------------------------------------
#    Utilitaire‚ÄØ: classification LLM ‚Äúg√©n√©rale/pr√©cise‚Äù + score de confiance
# ---------------------------------------------------------------------------
def classify_question_with_score(question: str) -> Tuple[str, float]:
    """
    Utilise le LLM (Ollama) pour classifier une question‚ÄØ: 'g√©n√©rale' ou 'pr√©cise'
    Retourne aussi un score de confiance [0,1].
    """
    prompt = f"""[INST]
Tu es un assistant IA. Ton r√¥le est de classer une question utilisateur en deux cat√©gories :
- "g√©n√©rale" : la question demande une vue d‚Äôensemble, une liste de documents ou un th√®me transversal.
- "pr√©cise" : la question cherche une information pr√©cise √† l‚Äôint√©rieur d‚Äôun seul document.

QUESTION :
{question}

Renvoie uniquement une r√©ponse JSON valide comme ceci :
{{
  "type": "g√©n√©rale" ou "pr√©cise",
  "confiance": nombre entre 0 et 1
}}
[/INST]"""

    try:
        response = generate_ollama(prompt=prompt, num_predict=120).strip()
        data = json.loads(response)
        q_type = data.get("type", "").lower()
        confiance = float(data.get("confiance", 0.0))
        if q_type in ["g√©n√©rale", "precise", "pr√©cise"]:
            return (q_type, round(confiance, 3))
    except Exception as e:
        logger.error(f"Erreur classification LLM : {e}")
    return ("pr√©cise", 0.0)  # fallback‚ÄØ: on consid√®re pr√©cise


# ---------------------------------------------------------------------------
#     Utilitaire‚ÄØ: charger tous les blocs d‚Äôun job_id (fichier par PDF)
# ---------------------------------------------------------------------------
def load_all_blocks(entreprise: str, job_id: str) -> List[Tuple[str, Dict]]:
    folder_path = os.path.join("cache_json", "save_summaryblocks", entreprise, job_id)
    logger.debug(f"Chargement des blocs depuis : {folder_path}")
    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"Dossier introuvable : {folder_path}")

    blocks: List[Tuple[str, Dict]] = []
    for filename in sorted(os.listdir(folder_path)):
        if not filename.startswith("bloc_") or not filename.endswith(".json"):
            continue
        full_path = os.path.join(folder_path, filename)
        try:
            with open(full_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            summary = data.get("summary", "")
            embedding = data.get("embedding")
            if embedding is None:
                logger.warning(f"Pas d'embedding pour {filename}, bloc ignor√©.")
                continue
            meta = {
                "source": filename,
                "score": data.get("score", 0),
                "translated": data.get("translated", False),
                "embedding": embedding
            }
            blocks.append((summary, meta))
        except Exception as e:
            logger.error(f"Erreur lecture {filename}: {e}")
            continue
    return blocks


# ---------------------------------------------------------------------------
#     Utilitaire‚ÄØ: trouver les blocs les plus pertinents (pipeline IA PDF)
# ---------------------------------------------------------------------------
def find_relevant_blocks(
    question: str,
    blocks: List[Tuple[str, Dict]],
    top_k: int = 5,
    relevance_threshold: float = 0.4
) -> List[Dict]:
    candidate_blocks = []
    embeddings = []
    scores_debug = []

    for summary, meta in blocks:
        emb = meta.get("embedding")
        if emb is not None:
            embeddings.append(emb)
            candidate_blocks.append((summary, meta))

    if not embeddings:
        return []

    block_embs = torch.tensor(np.stack(embeddings), dtype=torch.float32).to("cpu")
    question_emb = model.encode(question, convert_to_tensor=True).to(torch.float32).to("cpu")
    similarities = util.pytorch_cos_sim(question_emb, block_embs)[0]

    scored = []
    for i, (_, meta) in enumerate(candidate_blocks):
        sim = similarities[i].item()
        quality = meta.get("score", 0)
        combined = 0.7 * sim + 0.3 * quality
        scores_debug.append({
            "filename": meta["source"],
                        "sim_score": round(sim, 4),
            "quality": round(quality, 4),
            "combined": round(combined, 4)
        })
        if combined >= relevance_threshold:
            scored.append((i, combined))

    if not scored:
        scored = sorted(
            [(i, 0.7 * similarities[i].item() + 0.3 * candidate_blocks[i][1].get("score", 0))
             for i in range(len(candidate_blocks))],
            key=lambda x: x[1], reverse=True
        )[:top_k]
    else:
        scored.sort(key=lambda x: x[1], reverse=True)
        scored = scored[:top_k]

    pre_top = [i for i, _ in scored]
    cross_inputs = [(question, candidate_blocks[i][0]) for i in pre_top]
    rerank_scores = reranker.predict(cross_inputs)
    reranked = sorted(zip(pre_top, rerank_scores), key=lambda x: x[1], reverse=True)[:top_k]

    result = []
    for rank, (idx, rerank_score) in enumerate(reranked, 1):
        summary, meta = candidate_blocks[idx]
        block_score_debug = scores_debug[idx]
        logger.info(
            f"ASK Bloc s√©lectionn√© (RANK {rank}) : {meta['source']} | sim={block_score_debug['sim_score']}, "
            f"quality={block_score_debug['quality']}, combined={block_score_debug['combined']} | rerank_score={round(float(rerank_score),4)}"
        )
        result.append({
            "text": summary,
            "source": meta.get("source"),
            "debug_scores": block_score_debug
        })
    return result

# ---------------------------------------------------------------------------
#    Utilitaire‚ÄØ: prompt pour synth√®se multi-documents (mode g√©n√©rale)
# ---------------------------------------------------------------------------
def build_summary_prompt_from_metadata(question: str, docs: list) -> str:
    doc_lines = []
    for filename, resume in docs:
        doc_lines.append(f"- {filename}: {resume[:120]}...")
    docs_str = "\n".join(doc_lines)
    return f"""[INST]
Voici la question de l'utilisateur :
{question}

Voici la liste des documents pertinents avec leur r√©sum√© :
{docs_str}

R√©ponds par une synth√®se claire¬†:
- Si la question concerne plusieurs documents, cite-les bri√®vement.
- Si le sujet n‚Äôest trait√© dans aucun document, indique-le poliment.
- 2 √† 3 phrases maximum.
[/INST]"""

# ---------------------------------------------------------------------------
#    Utilitaires prompts (PDF pipeline)
# ---------------------------------------------------------------------------
def build_prompt(question: str, selected_blocks: List[Dict]) -> str:
    parts = [f"--- Bloc {i+1} ({b['source']}) ---\n{b['text']}" for i, b in enumerate(selected_blocks)]
    context = "\n\n".join(parts)
    return f"""[INST]
Tu es un assistant IA expert et synth√©tique. R√©ponds en deux √† trois phrases claires et pr√©cises √† la question suivante, en t'appuyant exclusivement sur les extraits fournis.

QUESTION :
{question}

EXTRAITS :
{context}

INSTRUCTIONS :
- R√©ponds en deux √† trois phrases maximum, sans introduction, ni structure acad√©mique.
- Si l‚Äôinformation n‚Äôest pas pr√©sente, dis-le poliment (‚ÄúNon mentionn√© dans le document.‚Äù).
- Corrige les √©ventuelles fautes dans la question.
[/INST]"""

def build_reformulation_prompt(question: str, selected_blocks: List[Dict]) -> str:
    parts = [f"--- Bloc {i+1} ({b['source']}) ---\n{b['text']}" for i, b in enumerate(selected_blocks)]
    context = "\n\n".join(parts)
    return f"""[INST]
Tu es un assistant IA expert et synth√©tique. L‚Äôutilisateur demande une reformulation de ta r√©ponse pr√©c√©dente √† la m√™me question.
R√©dige une r√©ponse diff√©rente, uniquement √† partir des extraits fournis, sans te r√©p√©ter.

QUESTION :
{question}

EXTRAITS :
{context}

R√àGLES :
- 1 √† 3 phrases claires, sans pr√©ambule.
- Ne redis pas la m√™me chose.
- Si l‚Äôinformation n‚Äôest pas pr√©sente, dis-le poliment (‚ÄúNon mentionn√© dans le document.‚Äù).
[/INST]"""

# ---------------------------------------------------------------------------
#    FONCTION CENTRALE : G√©n√©ration de la r√©ponse IA
# ---------------------------------------------------------------------------
def generate_answer(
    question: str,
    blocks: List[Tuple[str, Dict]],
    job_id: str = None,
    session_id: str = None,
    user_id: str = None,
    entreprise: str = "Entreprise_S3_Test",
    reformule: bool = False,
    general: bool = False
) -> str:
    total_start = time.time()

    # --- 1. Classifier la question (g√©n√©rale ou pr√©cise) ---
    q_type, confiance = classify_question_with_score(question)
    logger.info(f"üß† Type de question d√©tect√© : {q_type.upper()} (confiance={confiance})")

    if q_type == "g√©n√©rale":
        logger.info("üü° Branche G√âN√âRALE ‚Äî recherche via m√©tadonn√©es uniquement")
        results = find_documents_by_keyword(question, entreprise, job_id)
        if not results:
            return "Aucun document ne correspond √† cette th√©matique."

        prompt = build_summary_prompt_from_metadata(question, results)
        gen_start = time.time()
        answer = generate_ollama(
            prompt=prompt,
            num_predict=400,
            models=["llama3:instruct"],
            temperature=0.4
        ).strip()
        gen_time = time.time() - gen_start
        logger.info(f"‚è±Ô∏è Temps g√©n√©ration (g√©n√©rale) : {gen_time:.2f}s")
        blocks_used = [doc[0] for doc in results]  # liste des filenames

    else:
        logger.info("üîµ Branche PR√âCISE ‚Äî recherche vectorielle du PDF cible")
        question_emb = model.encode(question).tolist()
        pdf_filename = find_nearest_pdf_by_embedding(question_emb, entreprise, job_id)

        if not pdf_filename:
            logger.warning("Aucun document pertinent trouv√© via embeddings.")
            return "Je n‚Äôai pas trouv√© de document correspondant √† votre question."

        logger.info(f"üìÇ Document s√©lectionn√© via m√©tadonn√©es : {pdf_filename}")

        # Charger uniquement les blocs li√©s √† ce PDF
        all_blocks = load_all_blocks(entreprise, job_id)
        filtered_blocks = [
            b for b in all_blocks if pdf_filename in b[1].get("source", "")
        ]

        selected = find_relevant_blocks(question, filtered_blocks)
        retrieval_time = time.time() - total_start
        logger.info(f"ASK ‚è±Ô∏è Temps s√©lection blocs (retrieval+r√©rank) : {retrieval_time:.2f}s")

        if not selected:
            logger.info("üß† Aucun bloc pertinent ‚Äî pas d'information.")
            return "Je n‚Äôai pas trouv√© cette information dans les documents analys√©s."

        logger.info(f"üß† Prompt d√©clench√© : üìÑ PDF ({'reformulation' if reformule else 'standard'})")
        prompt = build_reformulation_prompt(question, selected) if reformule else build_prompt(question, selected)
        gen_start = time.time()
        answer = generate_ollama(
            prompt=prompt,
            num_predict=550,
            models=["llama3:instruct"],
            temperature=0.3
        ).strip()
        gen_time = time.time() - gen_start
        logger.info(f"‚è±Ô∏è Temps g√©n√©ration (pdf) : {gen_time:.2f}s")
        blocks_used = [b["source"] for b in selected]

    total_time = time.time() - total_start
    logger.info(f"‚úÖ R√©ponse g√©n√©r√©e en {total_time:.2f}s (total)")

    if session_id is None:
        session_id = str(uuid.uuid4())
    try:
        save_interaction(
            session_id=session_id,
            question=question,
            answer=answer,
            blocks_used=blocks_used,
            job_id=job_id,
            user_id=user_id
        )
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde de l'interaction chat : {e}")

    return answer

# ---------------------------------------------------------------------------
# FIN DU MODULE ask_engine.py
# ---------------------------------------------------------------------------


