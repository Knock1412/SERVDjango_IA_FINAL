import os
import json
import logging
import uuid
import torch
import numpy as np
import time
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer, util, CrossEncoder
from duckduckgo_search import DDGS
from ia_backend.services.ollama_gateway import generate_ollama
from ia_backend.services.chat_memory import save_interaction

logger = logging.getLogger(__name__)

model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def is_general_question(question: str) -> bool:
    import re
    patterns = [
        r"\b(quelle est|quand|combien|o√π|qui|pourquoi|comment|d√©finir|d√©finition de|loi|date)\b",
        r"\b(c‚Äôest quoi|peux[- ]tu m‚Äôexpliquer|explique moi|histoire de)\b",
        r"\b(m√©t√©o|heure|jour|capital[e]?|pr√©sident|actualit√©s?)\b",
    ]
    question = question.lower()
    return any(re.search(p, question) for p in patterns)

def search_web_duckduckgo(query: str, num_results: int = 5) -> List[str]:
    results = []
    with DDGS() as ddgs:
        for r in ddgs.text(query, max_results=num_results):
            if "body" in r:
                results.append(f"{r['title']}: {r['body']}")
    return results

def build_web_summary_prompt(question: str, results: List[str]) -> str:
    sources = "\n\n".join(f"- {res}" for res in results)
    return f"""[INST]
Tu es un assistant IA g√©n√©raliste. R√©ponds en deux √† trois phrases claire et pr√©cise √† la question suivante, en t'appuyant exclusivement sur le r√©sultat web.

Question : {question}

R√©sultats trouv√©s :
{sources}

Consignes :
- R√©ponds en deux √† trois phrases maximum, sans introduction, ni structure acad√©mique.
- Synth√©tise uniquement les infos pertinentes
- Ne sp√©cule pas. Si pas d'info claire, dis-le.
- Corrige les √©ventuelles fautes dans la question.
[/INST]"""

def load_all_blocks(entreprise: str, job_id: str) -> List[Tuple[str, Dict]]:
    folder_path = os.path.join("cache_json", "save_summaryblocks", entreprise, job_id)
    logger.debug(f"Chargement des blocs depuis : {folder_path}")
    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"Dossier introuvable : {folder_path}")

    blocks: List[Tuple[str, Dict]] = []
    for filename in sorted(os.listdir(folder_path)):
        if not filename.startswith("bloc_") or not filename.endswith(".json"):
            continue
        full_path = os.path.join(folder_path, filename)
        try:
            with open(full_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            summary = data.get("summary", "")
            embedding = data.get("embedding")
            if embedding is None:
                logger.warning(f"Pas d'embedding pour {filename}, bloc ignor√©.")
                continue
            meta = {
                "source": filename,
                "score": data.get("score", 0),
                "translated": data.get("translated", False),
                "embedding": embedding
            }
            blocks.append((summary, meta))
        except Exception as e:
            logger.error(f"Erreur lecture {filename}: {e}")
            continue
    return blocks

def find_relevant_blocks(
    question: str,
    blocks: List[Tuple[str, Dict]],
    top_k: int = 5,
    relevance_threshold: float = 0.4
) -> List[Dict]:
    candidate_blocks = []
    embeddings = []
    scores_debug = []

    for summary, meta in blocks:
        emb = meta.get("embedding")
        if emb is not None:
            embeddings.append(emb)
            candidate_blocks.append((summary, meta))

    if not embeddings:
        return []

    block_embs = torch.tensor(np.stack(embeddings), dtype=torch.float32).to("cpu")
    question_emb = model.encode(question, convert_to_tensor=True).to(torch.float32).to("cpu")
    similarities = util.pytorch_cos_sim(question_emb, block_embs)[0]

    scored = []
    for i, (_, meta) in enumerate(candidate_blocks):
        sim = similarities[i].item()
        quality = meta.get("score", 0)
        combined = 0.7 * sim + 0.3 * quality
        scores_debug.append({
            "filename": meta["source"],
            "sim_score": round(sim, 4),
            "quality": round(quality, 4),
            "combined": round(combined, 4)
        })
        if combined >= relevance_threshold:
            scored.append((i, combined))

    if not scored:
        scored = sorted(
            [(i, 0.7 * similarities[i].item() + 0.3 * meta.get("score", 0))
             for i, (_, meta) in enumerate(candidate_blocks)],
            key=lambda x: x[1], reverse=True
        )[:top_k]
    else:
        scored.sort(key=lambda x: x[1], reverse=True)
        scored = scored[:top_k]

    pre_top = [i for i, _ in scored]
    cross_inputs = [(question, candidate_blocks[i][0]) for i in pre_top]
    rerank_scores = reranker.predict(cross_inputs)
    reranked = sorted(zip(pre_top, rerank_scores), key=lambda x: x[1], reverse=True)[:top_k]

    result = []
    for rank, (idx, rerank_score) in enumerate(reranked, 1):
        summary, meta = candidate_blocks[idx]
        block_score_debug = scores_debug[idx]
        logger.info(
            f"ASK Bloc s√©lectionn√© (RANK {rank}) : {meta['source']} | sim={block_score_debug['sim_score']}, "
            f"quality={block_score_debug['quality']}, combined={block_score_debug['combined']} | rerank_score={round(float(rerank_score),4)}"
        )
        result.append({
            "text": summary,
            "source": meta.get("source"),
            "debug_scores": block_score_debug
        })
    return result

def build_prompt(question: str, selected_blocks: List[Dict]) -> str:
    parts = [f"--- Bloc {i+1} ({b['source']}) ---\n{b['text']}" for i, b in enumerate(selected_blocks)]
    context = "\n\n".join(parts)
    return f"""[INST]
Tu es un assistant IA expert et synth√©tique. R√©ponds en deux √† trois phrases claire et pr√©cise √† la question suivante, en t'appuyant exclusivement sur les extraits fournis.

QUESTION :
{question}

EXTRAITS :
{context}

INSTRUCTIONS :
- R√©ponds en deux √† trois phrases maximum, sans introduction, ni structure acad√©mique.
- Si l‚Äôinformation n‚Äôest pas pr√©sente, dis-le poliment (‚ÄúNon mentionn√© dans le document.‚Äù).
- Corrige les √©ventuelles fautes dans la question.
[/INST]"""

def build_reformulation_prompt(question: str, selected_blocks: List[Dict]) -> str:
    parts = [f"--- Bloc {i+1} ({b['source']}) ---\n{b['text']}" for i, b in enumerate(selected_blocks)]
    context = "\n\n".join(parts)
    return f"""[INST]
Tu es un assistant IA expert et synth√©tique. L‚Äôutilisateur demande une reformulation de ta r√©ponse pr√©c√©dente √† la m√™me question.
R√©dige une r√©ponse diff√©rente, uniquement √† partir des extraits fournis, sans te r√©p√©ter.

QUESTION :
{question}

EXTRAITS :
{context}

R√àGLES :
- 1 √† 3 phrases claires, sans pr√©ambule.
- Ne redis pas la m√™me chose.
- Si l‚Äôinformation n‚Äôest pas pr√©sente, dis-le poliment (‚ÄúNon mentionn√© dans le document.‚Äù).
[/INST]"""

def generate_answer(
    question: str,
    blocks: List[Tuple[str, Dict]],
    job_id: str = None,
    session_id: str = None,
    user_id: str = None,
    reformule: bool = False,
    general: bool = False
) -> str:
    total_start = time.time()

    # 1. D'abord, d√©tecte si c'est une question g√©n√©rale
    if general or is_general_question(question):
        logger.info("üß† Prompt d√©clench√© : üåê Web (DuckDuckGo)")
        web_results = search_web_duckduckgo(question)
        if not web_results:
            return "D√©sol√©, je n'ai rien trouv√© sur le web."
        prompt = build_web_summary_prompt(question, web_results)
        gen_start = time.time()
        answer = generate_ollama(
            prompt=prompt,
            num_predict=400,
            models=["llama3:instruct"],
            temperature=0.4
        ).strip()
        gen_time = time.time() - gen_start
        logger.info(f"‚è±Ô∏è Temps g√©n√©ration (web) : {gen_time:.2f}s")
        blocks_used = []

    else:
        # 2. Sinon, logique classique PDF
        selected = find_relevant_blocks(question, blocks)
        retrieval_time = time.time() - total_start
        logger.info(f"ASK ‚è±Ô∏è Temps s√©lection blocs (retrieval+r√©rank) : {retrieval_time:.2f}s")

        if not selected:
            logger.info("üß† Aucun bloc pertinent ‚Äî pas d'information.")
            return "Je n‚Äôai pas trouv√© cette information dans les documents analys√©s."

        logger.info(f"üß† Prompt d√©clench√© : üìÑ PDF ({'reformulation' if reformule else 'standard'})")
        prompt = build_reformulation_prompt(question, selected) if reformule else build_prompt(question, selected)
        gen_start = time.time()
        answer = generate_ollama(
            prompt=prompt,
            num_predict=550,
            models=["llama3:instruct"],
            temperature=0.3
        ).strip()
        gen_time = time.time() - gen_start
        logger.info(f"‚è±Ô∏è Temps g√©n√©ration (pdf) : {gen_time:.2f}s")
        blocks_used = [b["source"] for b in selected]

    total_time = time.time() - total_start
    logger.info(f"‚úÖ R√©ponse g√©n√©r√©e en {total_time:.2f}s (total)")

    if session_id is None:
        session_id = str(uuid.uuid4())
    try:
        save_interaction(
            session_id=session_id,
            question=question,
            answer=answer,
            blocks_used=blocks_used,
            job_id=job_id,
            user_id=user_id
        )
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde de l'interaction chat : {e}")

    return answer
